\documentclass[12pt]{article}
\usepackage{amsthm, amssymb, enumitem, parskip, mathptmx}
\newtheorem{definition}{Definition}
\newtheorem{axiom}{Axiom}
\renewcommand{\thesection}{\Roman{section}.}
\usepackage{titlesec} %section formatting
\titleformat*{\section}{\normalfont\bfseries}
\titleformat*{\subsection}{\normalfont\bfseries}
\usepackage[margin=1in]{geometry}
\usepackage[doublespacing]{setspace}
\raggedright
\setlength{\parindent}{0.5in}
\sloppy
\usepackage[all]{nowidow} %avoid widows
\usepackage{fancyhdr} %page number formatting
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyhead[R]{\thepage}
\usepackage[bottom]{footmisc} %references
\usepackage[notes,backend=bibtex]{biblatex-chicago}
\addbibresource{refs.bib}
\usepackage{titling}
\renewcommand{\maketitle}{%
    \begin{titlepage}
        \begin{center}
            \vfill
            \vspace*{\baselineskip}
            \vfill
            \textbf{Normalisation for Ordinary Agents: A Solution to the Problem of Logical Omniscience}
            \vfill
            Choo Ying Hua, David\\
            PE3101P: Decision and Social Choice\\
            17 November, 2023\\
            Word Count: 2000
            \vfill
        \end{center}
    \end{titlepage}
    }
\begin{document}
\maketitle
\section{Introduction}
Bayesianism, hereafter the Bayesian model, is the interpretation of probability as a quantification of degrees of belief. Because probabilities must adhere to the Kolmogorov axioms, it follows from the axiom of normalisation that a rational agent must believe all logical truths, even those unknown. This impossible demand on rationality is the problem of logical omniscience.

This paper will discuss the problem and demonstrate that its conclusion is problematic because it demands us to abandon the valuable Bayesian model despite the lack of equally valuable alternatives. I will then argue for a pragmatic solution to the problem by relaxing the axiom of normalisation, insofar as a rational agent must assign a maximum degree of belief to logical truths only if they can be justified to be known. This solution is most ideal because it directly addresses the impossible demand posed by the problem, while also preserving the normative value of the model and avoiding complications that render it impractical.

\section{The Problem of Logical Omniscience}
In the Bayesian model, an agent is rational only if their degrees of belief in propositions, hereafter credences, satisfy the mathematical conditions of a probability function.\autocite[13]{bdrc} These conditions are the Kolmogorov axioms which state that, for any set of propositions $S$, the credence function $Cr:S\rightarrow\mathbb{R}$ must satisfy the following:
\begin{axiom}
    (Non-negativity) For any proposition $A\in S$, one has $Cr(A)\geq0$.
\end{axiom}
\begin{axiom}
    (Normalisation) For any logical truth $T\in S$, one has $Cr(T)=1$.
\end{axiom}
\begin{axiom}
    (Finite additivity) For any propositions $A_1, A_2, ..., A_n\in S$ such that for integers $j,k\in[1,n]$ where $j\neq k$ and $Cr(A_j\land A_k)=0$, one has $Cr(\bigvee_{i=1}^{n}A_i)=\sum_{i=1}^{n}Cr(A_i)$.
\end{axiom}
What if $S$ includes propositions whose truth we have yet to ascertain? Even for such propositions, the axiom of normalisation demands that an ordinary agent must assign maximum credence to its truth to be deemed rational. However, it is unreasonable to qualify rationality on knowledge of truths that ordinary agents cannot prove conclusively. Consider the case of Fermat's Last Theorem, which took over three hundred years to prove, and only successfully so with cutting-edge mathematics that would be inaccessible to an ordinary agent.\footnote{For background, the theorem states that for any $n\in\mathbb{Z}_{\geq2}$, the equation $a^n+b^n=c^n$ has no positive integer solutions. The theorem seems innocuous, but it was a notorious unsolved problem that required mathematics developed after Fermat's time to prove conclusively.} Is it fair to consider someone in the past to be irrational if they did not assign maximum credence in the truth of the theorem?\autocite[108]{dogramaci} This clearly impossible demand is the problem of logical omniscience, which may be formalised as:\autocite{youtube}
\begin{enumerate}[label=\textbf{P\arabic*:},leftmargin=0.5in]
    \item No ordinary agent can have credences in an infinite number of propositions.
    \item There are an infinite number of logical truths.
    \item If the Bayesian model is applicable to ordinary agents, then any ordinary agent must have maximum credence in any logical truth.
    \item No ordinary agent can have credences in all logical truths.
\end{enumerate}
\begin{enumerate}[resume,label=\textbf{C:}, topsep=0pt, leftmargin=0.5in]
    \item The Bayesian model is inapplicable to ordinary agents.
\end{enumerate}

I will demonstrate that the argument is sound. Suppose there exists an ordinary agent who knows an infinite number of propositions, so they must have successfully enumerated through an infinite set of propositions and determined each individual credence. However, it is nigh impossible for an ordinary agent to even begin constructing this set, much less to enumerate through it. This contradiction proves \textbf{P1}.

Additionally, let the greatest number of logical truths be a finite number $n$, so the set of logical truths is $T=\{T_1,T_2,...,T_n\}$. For any $i$ with $1\leq i\leq n$, we know $T_i\in T$ is true, so the tautology $(T_i\lor\lnot T_i)$ is also a logical truth. This additional logical truth means that there are at least $n+1$ logical truths, so this contradicts the definition of $n$ as the greatest number of logical truths. Since there is no upper bound on the number of logical truths, there must be an infinite number of logical truths, so \textbf{P2} is true.

Consequently, \textbf{P3} restates the axiom of normalisation. \textbf{P4} also follows naturally: if there are an infinite number of logical truths (\textbf{P1}) and no ordinary agent can have credences in an infinite number of propositions (\textbf{P2}), then \textbf{P4} must be true. Hence, the conclusion is obtained by \textit{modus tollens} of \textbf{P4} on \textbf{P3}.

The problem of this conclusion is that we are compelled to discard a valuable normative model of rationality on the basis of a technicality, even though we lack viable alternatives. The conclusion that the model is inapplicable to any ordinary agent implies that it is pointless to evaluate rationality on the Bayesian model. This cannot be the case, because the model is useful and widely-applicable, and valuable for its balance between rigour and intuition. It literally pays to abide by the model, since we can show by way of a simple Dutch Book argument that if an agent assigns their credences in violation of the axioms, then they may be duped into buying bets which lead to a guaranteed loss.\autocite[44]{bdrc} Hence, by abandoning the model just because ordinary agents cannot fulfil the impossible demand of logical omniscience, we forgo a definition of rationality grounded in easily-verifiable principles in favour of either a more laissez-faire approach to defining rationality or the sweeping generalisation that any ordinary agents is irrational. The latter is intuitively repulsive, and the former cannot be the right approach since it is reasonable to expect normative claims of rationality to be grounded in objectively measurable standards. Therefore, it is preferable to preserve the utility of the model by attempting to reconcile the problem of logical omniscience with the model, rather than give the model up.

\section{Reformulating Normalisation}
To account for the shortcomings of the Bayesian model, we can simply accept that the model only prescribes how a logically omniscient agent must assign their credences. The axioms provide a starting point for further amendments to make them more reflective of how ordinary agents behave. Since the current formulation of the axiom of normalisation is the critical flaw of the model, we could reformulate it to:
\begin{axiom}
    (Weak Normalisation, WN) For any \textbf{known} logical truth $T\in S$, one has $Cr(T)=1$.
\end{axiom}
This reformulation implies that an ordinary agent ought to have maximum credence in a logical truth, only if they know that it is one.\autocite{sep} To know a logical truth would mean that an agent is able to justify its truth or non-falsity. Consequently, they must assign maximum credence to the truth of this proposition to qualify as rational.

This reformulation is most desirable because it renders truths that are unknown to the agent to be irrelevant to considerations of rationality, so it resolves the demand of logical omniscience. Furthermore, it upholds the normative standards of the Bayesian model, as the demand for an agent to justify their credences is reasonable. Therefore, applying WN to the example of Fermat's Last Theorem, the ordinary agent in the past remains rational for having a non-maximum credence in its truth, because prior to its proof it would have been impossible for anyone to produce a valid reason for their credence in it. However, with the proof of the theorem, even if a rational agent does not understand the proof, they must assign maximum credence in its truth because they can ground their credence in it on the authority and rigorous review that the proof should have underwent. Hence, this constitutes a more pragmatic and reflective approach to modelling the credences of ordinary agents.

A potential challenge to this approach is that invoking an arbitrary criteria to ``know'' a logical truth would taint an otherwise objective Bayesian model with subjectivity. Agents may utilise different criteria to determine what counts as knowledge, so privileging the definition of knowledge as justified true belief may be seen as arbitrary. Since the model is grounded in this potentially contentious perspective, this objection would yet again undermine its normative value. However, I contend that reformulating the axioms in this manner does not demand universal agreement on a definition of knowledge. It is beyond the scope of the Bayesian model to account for this underlying epistemological challenge, since the model only aims to provide a principled framework for rational credence assignment. The reformulation of the axiom accommodates for the diversity of perspectives in defining knowledge by limiting its demands to conditions that rational agents ought to fulfil, and it remains agnostic on a specific definition of knowledge. Thus, subjectivity is not introduced by the reformulation and the value of the model is preserved in a valid manner.

\section{Another Reformulation?}
In reformulating the axioms, the natural question that follows would be: how far should we go to make the model more reflective of any ordinary agent's credences? I argue that if we wish to prevent complications from arising, then the limit must be the WN formulation, as going any further would totally destabilise the mathematical foundations of the system of probability. By relaxing the axioms to compensate for logical non-omniscience, we already lose some applications to probability that make the Bayesian model appealing.\autocite[436]{oup} In particular, results that hinge upon the precision offered by the axioms become contradictory or nonsensical if the reformulations become excessive.

To illustrate this, suppose we take the view that credences need not be single real values, but are a real interval. This is not implausible, because ordinary agents may account for a range of uncertainty or indifference that is more faithful to their epistemic position. Therefore, if one knows a logical truth, their credence ought to be an interval that includes the number one.\autocite{sep} We may thus reformulate the axiom of normalisation to:
\begin{axiom}
    (Very Weak Normalisation, VWN) For any known logical truth $T\in S$, there exists $c$ with $0\leq c\leq 1$ such that $c\leq Cr(T)\leq1$.
\end{axiom}
However, this causes some useful results in probability to become undone. Consider the case of conditional credences, defined as:
\begin{definition}
    For propositions $A,B\in S$ with $Cr(B)>0$, one has $Cr(A|B)=\frac{Cr(A\land B)}{Cr(B)}$.
\end{definition}
Intuitively, this represents an agent's credence that a proposition $A$ is true, supposing that $B$ is true.\autocite[32]{bdrc} If we take $B$ to be a known logical truth, by WN we have that $Cr(B)=1$, so $Cr(A|B)=Cr(A\land B)$. Also, because the truth of the proposition $(A\land B)$ depends solely on $A$, we know that $Cr(A\land B)=Cr(A)$. Thus we obtain a well-defined conditional credence. However, in VWN, $Cr(B)$ may be a real interval, so considering conditional credences becomes mathematically absurd because it is impossible to divide $Cr(A\land B)$ over an interval.

Perhaps we accept that conditional credence as an interval is meaningful because it is still a credence, so we may pursue the logic in WN and let $Cr(A\land B)=a$ for some $0\leq a\leq1$. Dividing $Cr(A)$ over the bounds of $Cr(B)$, we get $Cr(A|B)=[a,\frac{a}{c}]$ for some $0\leq c\leq1$. However, if we let $a=1$ and $c\leq 1$, we get a conditional credence that may be greater than the maximum credence in a logical truth. This strange result violates our logical intuitions, because it suggests that in certain situations, there exist propositions that we may believe in more than logical truths. It is obviously inconceivable, if not outright impossible, to have ``more'' belief in a proposition than \textit{`A triangle has three angles'}. Therefore, VWN is too na\"ive, thus it cannot be the right approach to account for logical non-omniscience.

One could contend that the above examples are unfair because there could be methods that reconcile VWN with a consistent and improved Bayesian model. However, I argue that introducing further mathematical complexity causes the model to lose its intuitive appeal. As stated previously, the appeal of the model comes from the balance between rigour in its mathematical foundations and its simplicity.\footnote{See page 3.} Adding layers of sophistication to the model infringes upon its simplicity and imposes further implicit conditions for rationality. This results in a situation not much different from the impossible demands on rationality posed by the problem of logical omniscience, since a mathematically-refined Bayesian model could demand so much calculation from an ordinary agent that it may be impossible to ever evaluate if they are truly rational. Those advocating this solution will find themselves back where they started resulting from the mathematical complexity that they have introduced. Hence, reformulating the axiom excessively is not a pragmatic solution to the problem of logical omniscience, so the best solution is to simply relax the axioms to the WN formulation.
\section{Conclusion}
This paper has demonstrated that the best solution to the problem of logical omniscience is to adopt the WN formulation of normalisation and thus limit the assignment of maximum credence to logical truths that ordinary agents can justify. This is the only possible compromise to preserve the normative value of the Bayesian model, because adopting weaker formulations like WVN would destabilise the system of probability, and also cause impossible demands to present themselves again in the form of a model that is too convoluted to be useful for an ordinary agent.
\pagebreak
\printbibliography
\end{document}