\documentclass[12pt]{article}
\usepackage{amsthm, amssymb, enumitem, parskip, mathptmx}
\newtheorem{definition}{Definition}
\newtheorem{axiom}{Axiom}
\renewcommand{\thesection}{\Roman{section}.}
\usepackage{titlesec} %section formatting
\titleformat*{\section}{\normalfont\bfseries}
\titleformat*{\subsection}{\normalfont\bfseries}
\usepackage[margin=1in]{geometry}
\usepackage[doublespacing]{setspace}
\raggedright
\setlength{\parindent}{0.5in}
\sloppy
\usepackage[all]{nowidow} %avoid widows
\usepackage{fancyhdr} %page number formatting
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyhead[R]{\thepage}
\usepackage[bottom]{footmisc} %references
\usepackage[notes,backend=bibtex]{biblatex-chicago}
\addbibresource{refs.bib}
\usepackage{titling}
\renewcommand{\maketitle}{%
    \begin{titlepage}
        \begin{center}
            \vfill
            \vspace*{\baselineskip}
            \vfill
            \textbf{Normalisation for Ordinary Agents: A Solution to the Problem of Logical Omniscience}
            \vfill
            Choo Ying Hua, David\\
            PE3101P: Decision and Social Choice\\
            17 November, 2023\\
            Word Count: 2000
            \vfill
        \end{center}
    \end{titlepage}
    }
\begin{document}
\maketitle
\section{Introduction}
Bayesianism, hereafter the Bayesian model, is the interpretation of probability as a quantification of degrees of belief. Because probabilities must adhere to the Kolmogorov axioms, it follows from the axiom of normalisation that a rational agent must believe all logical truths, even those unknown. This impossible demand on rationality is the problem of logical omniscience.

This paper will discuss the problem and demonstrate that its conclusion is problematic because it demands us to abandon the Bayesian model despite the lack of equally valuable alternatives. I will then argue for a pragmatic solution to the problem by relaxing the axiom of normalisation, insofar as a rational agent must assign a maximum degree of belief to logical truths only if they can be justified to be known. This solution is most ideal because it directly addresses the impossible demand posed by the problem, while also preserving the utility of the model and avoiding complications that render it impractical.

\section{The Problem of Logical Omniscience}
In the Bayesian model, an agent is rational only if their degrees of belief in propositions, hereafter credences, satisfy the mathematical conditions of a probability function.\autocite[13]{bdrc} These conditions are the Kolmogorov axioms which state that, for any set of propositions $S$, the credence function $Cr:S\rightarrow\mathbb{R}$ must satisfy the following:
\begin{axiom}
    (Non-negativity) For any proposition $A\in S$, one has $Cr(A)\geq0$.
\end{axiom}
\begin{axiom}
    (Normalisation) For any logical truth $T\in S$, one has $Cr(T)=1$.
\end{axiom}
\begin{axiom}
    (Finite additivity) For any propositions $A_1, A_2, ..., A_n\in S$ such that for integers $j,k\in[1,n]$ where $j\neq k$ and $Cr(A_j\land A_k)=0$, one has $Cr(\bigvee_{i=1}^{n}A_i)=\sum_{i=1}^{n}Cr(A_i)$.
\end{axiom}
What if $S$ includes propositions whose truth we have yet to ascertain? Even for such propositions, the axiom of normalisation demands that an ordinary agent must assign maximum credence to its truth to be deemed rational. However, it is unreasonable to qualify rationality on knowledge of truths that ordinary agents cannot prove conclusively. Consider the case of Fermat's Last Theorem, which took over three hundred years to prove, and only successfully so with cutting-edge mathematics that would be inaccessible to an ordinary agent.\footnote{For background, the theorem states that for any $n\in\mathbb{Z}_{\geq2}$, the equation $a^n+b^n=c^n$ has no positive integer solutions. The theorem seems innocuous, but it was a notorious unsolved problem that required mathematics developed after Fermat's time to prove conclusively.} Is it fair to consider someone in the past to be irrational if they did not assign maximum credence in the truth of the theorem?\autocite[108]{dogramaci} This clearly impossible demand is the problem of logical omniscience, which may be formalised as:\autocite{youtube}
\begin{enumerate}[label=\textbf{P\arabic*:},leftmargin=0.5in]
    \item No ordinary agent can have credences in an infinite number of propositions.
    \item There are an infinite number of logical truths.
    \item If the Bayesian model is applicable to ordinary agents, then any ordinary agent must have maximum credence in any logical truth.
    \item No ordinary agent can have credences in all logical truths.
\end{enumerate}
\begin{enumerate}[resume,label=\textbf{C:}, topsep=0pt, leftmargin=0.5in]
    \item The Bayesian model is inapplicable to ordinary agents.
\end{enumerate}

I will demonstrate that the argument is sound. Suppose there exists an ordinary agent who knows an infinite number of propositions, so they must have successfully enumerated through an infinite set of propositions and determined each individual credence. However, it is nigh impossible for an ordinary agent to even begin constructing this set, much less to enumerate through it. This contradiction proves \textbf{P1}.

Additionally, let the greatest number of logical truths be a finite number $n$, so the set of logical truths is $T=\{T_1,T_2,...,T_n\}$. For any $i$ with $1\leq i\leq n$, we know $T_i\in T$ is true, so the tautology $(T_i\lor\lnot T_i)$ is also a logical truth. This additional logical truth means that there are at least $n+1$ logical truths, so this contradicts the definition of $n$ as the greatest number of logical truths. Since there is no upper bound on the number of logical truths, there must be an infinite number of logical truths, so \textbf{P2} is true.

Consequently, \textbf{P3} restates the axiom of normalisation. \textbf{P4} also follows naturally: if there are an infinite number of logical truths (\textbf{P1}) and no ordinary agent can have credences in an infinite number of propositions (\textbf{P2}), then \textbf{P4} must be true. Hence, the conclusion is obtained by \textit{modus tollens} of \textbf{P4} on \textbf{P3}.

The problem of this conclusion is that we are compelled to discard a valuable model of rationality on the basis of a technicality, even though we lack viable alternatives to replace it. The conclusion that the model is inapplicable to any ordinary agent implies that it is pointless to evaluate rationality using the Bayesian model. This cannot be the case, because the model is useful and widely-applicable, and valuable for its balance between rigour and intuition. It literally pays to abide by the model, since we can show by way of a simple Dutch Book argument that if an agent has credences that violate the axioms, then they may be duped into buying bets which lead to a guaranteed loss.\autocite[44]{bdrc} Hence, abandoning the model just because ordinary agents never attain the tall order of logical omniscience forgoes well-grounded criteria for rationality in favour of either a more laissez-faire approach in defining rationality, or the sweeping generalisation that any ordinary agent is irrational. The latter is intuitively repulsive, and the former cannot be the right approach, since a subjective criteria of rationality is not generalisable to any agent and hence meaningless as a framework for comparison. Therefore, we should preserve the utility of the model by attempting to reconcile it with the problem of logical omniscience, rather than give it up without a better substitute.

\section{Reformulating Normalisation}
To account for the shortcomings of the Bayesian model, we can simply accept that the model merely prescribes how a logically omniscient agent must assign their credences. Hence, to apply the model to ordinary agents, the axioms must be amended to reflect of how ordinary agents behave. Since the axiom of normalisation is the critical flaw that leads to the problem, we could reformulate it to:
\begin{axiom}
    (Weak Normalisation, WN) For any \textbf{known} logical truth $T\in S$, one has $Cr(T)=1$.
\end{axiom}
This reformulation implies that an ordinary agent ought to have maximum credence in a logical truth, only if they know that it is one.\autocite{sep} To know a logical truth would mean that an agent is able to justify its truth or non-falsity. Consequently, they must assign maximum credence to the truth of this proposition to qualify as rational.

This reformulation is most desirable because it renders truths that are unknown to an ordinary agent as irrelevant to considerations of rationality, so it eliminates the demand of logical omniscience. Furthermore, it is coherent with the demands on rationality prescribed by the model, as it is reasonable to expect that a rational agent is able to to justify their credences. Therefore, applying WN to the example of Fermat's Last Theorem, the ordinary agent in the past remains rational for having a non-maximum credence in its truth, because prior to its proof it would have been impossible for anyone to produce a valid reason for their credence in it. However, with the proof of the theorem, even if a rational agent does not understand the proof, they must assign maximum credence in its truth because they can ground their credence in it on the authority and rigorous review that the proof should have underwent. Hence, this constitutes a more pragmatic and reflective approach to modelling the credences of ordinary agents.

A potential challenge to this approach is that invoking an arbitrary criteria to ``know'' a logical truth would taint an otherwise objective Bayesian model with subjectivity. Agents may utilise different criteria to determine what counts as knowledge, so privileging the definition of knowledge as justified true belief may be seen as arbitrary. Since the model is grounded in this potentially contentious perspective, this objection would yet again undermine its utility. However, I contend that reformulating the axioms in this manner does not demand universal agreement on a definition of knowledge. It is beyond the scope of the Bayesian model to account for this underlying epistemological challenge, since the model only aims to provide a principled framework for rational credence assignment. The reformulation of the axiom accommodates for the diversity of perspectives in defining knowledge by limiting its demands to conditions that rational agents ought to fulfil, and it remains agnostic on a specific definition of knowledge. Thus, subjectivity is not introduced by the reformulation and the value of the model is preserved in a sound manner.

\section{Another Reformulation?}
In reformulating the axioms, the natural question that follows would be: how far should we go to make the model more reflective of any ordinary agent's credences? I argue that if we wish to prevent complications from arising, our limit must be the WN formulation, as going any further would totally destabilise the mathematical foundations for the system of probability. By relaxing the axioms to compensate for logical non-omniscience, we already lose some applications to probability that make the Bayesian model appealing.\autocite[436]{oup} In particular, results that hinge upon the precision offered by the axioms become contradictory or nonsensical if the reformulations become excessive.

To illustrate this view, suppose that credences need not be single real values, but are a real interval. This is plausible, because ordinary agents may account for a range of uncertainty or indifference that is more faithful to their epistemic position. Therefore, if a rational agent knows a logical truth, their credence ought to be an interval that includes the number one.\autocite{sep} We may thus reformulate the axiom of normalisation to:
\begin{axiom}
    (Very Weak Normalisation, VWN) For any known logical truth $T\in S$, there exists $c$ with $0\leq c\leq 1$ such that $c\leq Cr(T)\leq1$.
\end{axiom}
However, this causes some useful results in probability to become undone. Consider the case of conditional credences. Intuitively, this represents an agent's credence that a proposition $A$ is true, supposing that $B$ is true.\autocite[32]{bdrc} This is defined as:
\begin{definition}
    For propositions $A,B\in S$ with $Cr(B)>0$, one has $Cr(A|B)=\frac{Cr(A\land B)}{Cr(B)}$.
\end{definition}
If we take $B$ to be a known logical truth, by WN we have that $Cr(B)=1$, so $Cr(A|B)=Cr(A\land B)$. Also, the truth of the proposition $(A\land B)$ depends solely on $A$ as $B$ is logically true, so $Cr(A\land B)=Cr(A)$. Thus we obtain a well-defined conditional credence which is $Cr(A|B)=Cr(A)$. However, in VWN, $Cr(B)$ may be a real interval, so considering conditional credences becomes mathematically absurd for it is impossible to divide $Cr(A\land B)$ over an interval.

Perhaps we accept that conditional credences as intervals are still meaningful, so we compromise on mathematical rigour. We may pursue the logic of WN to let $Cr(A\land B)=a$ for some $0\leq a\leq1$, and divide $Cr(A)$ over the bounds of $Cr(B)$ to obtain $Cr(A|B)=[a,\frac{a}{c}]$ for some $0\leq c\leq1$. However, if we let $a=1$ and $c\leq 1$, we get a conditional credence that may be greater than the maximum credence in a logical truth. This result violates our logical intuitions, because it suggests that in certain situations, there exist propositions that we may believe in more than logical truths. It is inconceivable, if not outright impossible, to have ``more'' belief in a proposition than \textit{`A triangle has three angles'}. Therefore, VWN is too na\"ive, thus it cannot be the right approach to account for logical non-omniscience.

One could contend that the above examples are unfair because there could be methods that reconcile VWN with a consistent and improved Bayesian model. However, I argue that introducing further mathematical complexity causes the model to lose its intuitive appeal. As stated previously, the appeal of the model comes from the balance between rigour in its mathematical foundations and its simplicity.\footnote{See page 3.} Adding layers of sophistication to the model infringes upon its simplicity and imposes further implicit conditions for rationality. This results in a situation not much different from the impossible demands on rationality posed by the problem of logical omniscience, since a mathematically-refined Bayesian model could demand so much calculation from an ordinary agent that it may be impossible to ever evaluate if they are truly rational. Those advocating this solution will find themselves back where they started as a consequence of the unnecessary mathematical complexity that they have introduced. Hence, extreme reformulations are not a pragmatic solution to the problem of logical omniscience, so the best solution is to adopt a moderate approach espoused by the WN formulation.
\section{Conclusion}
This paper has demonstrated that the best solution to the problem of logical omniscience is to adopt the WN formulation of normalisation which limits the assignment of maximum credence only to logical truths that ordinary agents can justify. This is the only possible compromise to preserve the utility of the Bayesian model, because adopting weaker formulations like WVN destabilises the system of probability, and also causes other impossible demands on rationality to reappear as a model that is much too convoluted to be useful for an ordinary agent.
\pagebreak
\printbibliography
\end{document}