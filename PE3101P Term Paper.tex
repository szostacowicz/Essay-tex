\documentclass[12pt]{article}
\usepackage{amsthm, amssymb, enumitem, parskip, mathptmx}
\newtheorem{definition}{Definition}
\newtheorem{axiom}{Axiom}
\renewcommand{\thesection}{\Roman{section}.}
\usepackage{titlesec} %section formatting
\titleformat*{\section}{\normalfont\bfseries}
\titleformat*{\subsection}{\normalfont\bfseries}
\usepackage[margin=1in]{geometry}
\usepackage[doublespacing]{setspace}
\raggedright
\setlength{\parindent}{0.5in}
\sloppy
\usepackage[all]{nowidow} %avoid widows
\usepackage{fancyhdr} %page number formatting
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyhead[R]{\thepage}
\usepackage[bottom]{footmisc} %references
\usepackage[notes,backend=bibtex]{biblatex-chicago}
\addbibresource{refs.bib}
\usepackage{titling}
\renewcommand{\maketitle}{%
    \begin{titlepage}
        \begin{center}
            \vfill
            \vspace*{\baselineskip}
            \vfill
            \textbf{Normalisation for Ordinary Agents: A Solution to the Problem of Logical Omniscience}
            \vfill
            Choo Ying Hua, David\\
            PE3101P: Decision and Social Choice\\
            17 November, 2023\\
            Word Count: 2000
            \vfill
        \end{center}
    \end{titlepage}
    }
\begin{document}
\maketitle
\section{Introduction}
Bayesianism, hereafter the Bayesian model, is the interpretation of probability as a quantification of degrees of belief. Because probabilities must adhere to the Kolmogorov axioms, it follows from the axiom of normalisation that a rational agent must believe all logical truths, even those unknown. This impossible demand on rationality is the problem of logical omniscience.

This paper will discuss the problem and demonstrate that its conclusion compels us to abandon the Bayesian model, despite its value as a robust yet intuitive model. I will then advance a pragmatic solution to the problem by relaxing the axiom of normalisation, insofar as a rational agent must assign a maximum degree of belief to logical truths only if they can be known. This is the only possible solution that preserves the utility of the model, because relaxing the axioms any further would introduce complications that render the model impractical for an ordinary agent.

\section{The Problem of Logical Omniscience}
In the Bayesian model, an agent is rational only if their degrees of belief in propositions, hereafter credences, satisfy the mathematical conditions of a probability function.\autocite[13]{bdrc} These conditions are the Kolmogorov axioms which state that, for any set of propositions $S$, the credence function\footnote{The credence function $Cr$ assigns propositions in a set $S$ to some real number.} $Cr:S\rightarrow\mathbb{R}$ must satisfy the following:
\begin{axiom}
    (Non-negativity) For any proposition $A\in S$, one has $Cr(A)\geq0$.
\end{axiom}
\begin{axiom}
    (Normalisation) For any logical truth $T\in S$, one has $Cr(T)=1$.
\end{axiom}
\begin{axiom}
    (Finite Additivity) For any propositions $A_1, A_2,\dots, A_n\in S$ such that for integers $j,k\in[1,n]$ where $j\neq k$ and $Cr(A_j\land A_k)=0$,\footnote{In terms of the content taught in the module, this is a partition of propositions on a set $S$.} one has $Cr(\bigvee_{i=1}^{n}A_i)=\sum_{i=1}^{n}Cr(A_i)$.
\end{axiom}
However, what if $S$ includes propositions whose truth we have yet to ascertain? Unfortunately, in fully abiding by the axioms, the axiom of normalisation demands that, to be deemed rational, an ordinary agent must assign maximum credence to the truth of such unknown propositions. This is clearly unreasonable: why should we qualify rationality on knowledge of truths that ordinary agents cannot prove conclusively? Consider the case of Fermat's Last Theorem, which went unsolved for over three hundred years.\footnote{The theorem states that for any $n\in\mathbb{Z}_{\geq2}$, the equation $a^n+b^n=c^n$ has no positive integer solutions. This theorem seems innocuous, but its proof required extensive use of mathematics developed after Fermat's time, which would be inaccessible to an ordinary agent.} Is it fair to consider someone in the past as irrational if they did not assign maximum credence to the truth of the theorem?\autocite[108]{dogramaci} Hence, this impossible demand is the problem of logical omniscience, which may be formalised as:\autocite{youtube}
\begin{enumerate}[label=\textbf{P\arabic*:},leftmargin=0.5in]
    \item No ordinary agent can have credences in an infinite number of propositions.
    \item There are an infinite number of logical truths.
    \item If the Bayesian model is applicable to ordinary agents, then any ordinary agent must have maximum credence in any logical truth.
    \item No ordinary agent can have credences in all logical truths.
\end{enumerate}
\begin{enumerate}[resume,label=\textbf{C:}, topsep=0pt, leftmargin=0.5in]
    \item The Bayesian model is inapplicable to ordinary agents.
\end{enumerate}

I will demonstrate that the argument is sound. Suppose there exists an ordinary agent who knows an infinite number of propositions, which means that they have enumerated through an infinite set of propositions and assigned a credence to each proposition within the set. However, it is nigh impossible for an ordinary agent to construct this set, much less to enumerate through it, because they do not possess infinite time to do so. Hence, \textbf{P1} is true.

Next, suppose for the sake of contradiction that the greatest number of logical truths is a finite number $n$, so the set of logical truths is $T=\{T_1,T_2,\dots,T_n\}$. For any $i\in\mathbb{Z}$ with $1\leq i\leq n$, we know $T_i\in T$ is true, so the tautology $(T_i\lor\lnot T_i)$ is also a logical truth. This additional logical truth means that there are at least $n+1$ logical truths, which contradicts the definition of $n$ as the greatest number of logical truths. Since there is no upper bound on the number of logical truths, \textbf{P2} must be true.

Afterwards, \textbf{P3} restates the axiom of normalisation. \textbf{P4} also follows naturally: if there are an infinite number of logical truths (\textbf{P1}) and no ordinary agent can have credences in an infinite number of propositions (\textbf{P2}), then \textbf{P4} must be true. Hence, the conclusion is obtained by \textit{modus tollens} of \textbf{P4} on \textbf{P3}.

The issue with this conclusion is that it compels us to discard a valuable model of rationality on the basis of a technicality. The implication of the conclusion is that no ordinary agent can be rational, which suggests that there are only two mutually-exclusive choices that we may make to solve the problem of logical omniscience: either we abandon the Bayesian model because it is pointless to evaluate rationality on its criteria, or we retain the model but must accept that any ordinary agent is irrational. However, neither option is viable. We can demonstrate the value of the model by appealing to a Dutch Book argument: if an agent has credences that violate the axioms, then they may be duped into buying bets which lead to a guaranteed loss.\autocite[44]{bdrc} Though I acknowledge that the argument may be flawed in some respects,\footnote{For the reasons as explored in the module, including the view that the betting interpretation might implicitly change an agent's credences.} it is sufficient in demonstrating that it pays to abide by the model. The model should therefore be retained on account of its general usefulness from its balance between simplicity and rigour. Moreover, the second option is intuitively repulsive. It admits the sweeping generalisation that ordinary agents are never rational, but we know that they are capable of making calculations that demonstrate some degree of rationality, such as calculations of expected utility. Thus, we should not accept the false dichotomy of solutions suggested by the conclusion to the problem, and so it is necessary to find another solution which must both preserve the value of the model and account for the logical non-omniscience of ordinary agents.

\section{Reformulating Normalisation}
To save the Bayesian model, we could accept that it only prescribes how a logically omniscient agent must assign their credences. In order to apply the model to ordinary agents, the axioms must be duly amended to reflect how they actually behave. Since the axiom of normalisation is the critical flaw leading to the problem, we could reformulate it to:
\begin{axiom}
    (Weak Normalisation, WN) For any \textbf{known} logical truth $T\in S$, one has $Cr(T)=1$.
\end{axiom}
This reformulation implies that any ordinary, and also rational, agent ought to have maximum credence in $T$, only if they know that it is a logical truth.\autocite{sep} Typically, we say that an agent knows $T$, if they can justify its truth or equivalently, its non-falsity.

This reformulation is desirable because it renders truths unknown to an ordinary agent to be irrelevant for considerations of rationality, hence eliminating the demand of logical omniscience. Furthermore, it is coherent with the demands on rationality prescribed by the Bayesian model, since it is reasonable to expect that a rational agent can justify their credences. Therefore, applying WN to Fermat's Last Theorem, an ordinary agent in the past remains rational for having a non-maximum credence in its truth, because it would have been impossible for anyone to produce a valid justification for this credence. However, with the theorem now proven, even if a rational agent cannot reproduce the proof, they may justify their credence in the truth of the theorem on the grounds of the rigorous review that its proof underwent, so they must assign maximum credence to this truth. Thus, this constitutes a more pragmatic approach to modelling the credences of ordinary agents.

A possible challenge to this approach is that invoking an arbitrary criteria to ``justify'' a logical truth would taint the otherwise objective Bayesian model with subjectivity. What one agent may consider to be justification could be insufficient for another, so this introduces an element of subjectivity into considerations of rationality. This notion thus rejects the view that the model is universally applicable and therefore implicitly undermines its utility. I acknowledge this observation: after all, the Bayesian model aims to quantify degrees of belief, which are inherently subjective. However, I contend that reformulating the axioms in this manner does not demand universal agreement on a standard of justification. It is beyond the scope of the Bayesian model to account for this underlying epistemological challenge, as the model only aims to provide a principled framework for rational credence assignment. The WN formulation of the axiom accommodates the diversity of perspectives in defining justification by limiting its demands to conditions that rational agents ought to fulfil, so it remains agnostic on the specific requirements for justification. Hence, subjectivity is not introduced by the reformulation and we preserve the value of the model in a sound manner.

Consequently, the natural consideration that follows from any reformulation would be to determine the limits of further reformulations. To prevent complications from arising, I propose that this limit must be the WN formulation, because going further would completely destabilise the mathematical foundations of our conventional system of probability.\footnote{By this, I refer not just to results derived from the axioms, such as simple results like conditional probabilities, but even more sophisticated theorems or structures built on them, like distributions of random variables or the laws of large numbers. The sum total of these are our conventional system of probability which we apply for granted.}  By relaxing the axioms to compensate for logical non-omniscience, we lose some applications of probability that make the Bayesian model appealing.\autocite[436]{oup} Excessive reformulations are undesirable as they may yield contradictory or nonsensical results, especially for applications of probability that hinge upon the precision of the axioms, thus rendering such applications redundant.

To illustrate this, suppose credences need not be single real values, but are a real interval. This is plausible, because ordinary agents may account for a range of uncertainty or indifference that is more faithful to their epistemic position. Therefore, if a rational agent knows a logical truth, their credence ought to be an interval that includes the number one.\autocite{sep} We may then reformulate the axiom of normalisation to:
\begin{axiom}
    (Very Weak Normalisation, VWN) For any known logical truth $T\in S$, there exists $c$ with $0\leq c\leq 1$ such that $c\leq Cr(T)\leq1$.
\end{axiom}
However, this causes some useful results of probability to become undone. Consider the case of conditional credences. Intuitively, this represents an agent's credence that a proposition $A$ is true, supposing that $B$ is true.\autocite[32]{bdrc} This is defined as:
\begin{definition}
    For propositions $A,B\in S$ with $Cr(B)>0$, one has $Cr(A|B)=\frac{Cr(A\land B)}{Cr(B)}$.
\end{definition}
If we take $B$ to be a known logical truth, by WN we have that $Cr(B)=1$, so $Cr(A|B)=Cr(A\land B)$. Also, the truth of the proposition $(A\land B)$ depends solely on $A$, as $B$ is logically true, so $Cr(A\land B)=Cr(A)$. Thus, we obtain a well-defined conditional credence, which is $Cr(A|B)=Cr(A)$. However, in VWN, $Cr(B)$ may be a real interval, so considering conditional credences becomes mathematically absurd, for it is impossible to divide $Cr(A\land B)$ over an interval. This already undermines the case for relaxing the axioms in this manner.

Perhaps we still accept that conditional credences as intervals are meaningful,\footnote{We may view conditional credences as reflections of our belief in some propositions $A$, knowing that we are in a specific state of the world where some other proposition $B$ is true.} so we compromise on mathematical rigour. We may pursue the logic of WN to let $Cr(A\land B)=a$ for some $0\leq a\leq1$, and divide $Cr(A)$ over the bounds of $Cr(B)$ to obtain $Cr(A|B)=[a,\frac{a}{c}]$ for some $0\leq c\leq1$. However, if we let $a=1$ and $c\leq 1$, we get a conditional credence that may be greater than the maximum credence in a logical truth. This result violates our logical intuitions, because it suggests that in certain situations, there exist propositions that we may believe in more than logical truths. It is inconceivable, if not outright impossible, to have ``more'' belief in a proposition than \textit{`A triangle has three angles'}. Therefore, VWN is too na\"ive, so it cannot be the right approach to account for logical non-omniscience.

One could contend that the above examples are unfair because there could be methods that reconcile VWN with a more robust Bayesian model. However, it is possible that such methods are too complex to be useful for an ordinary agent. It is already challenging enough to qualify an ordinary agent's rationality on the Bayesian model, since it demands that an agent must have complete\footnote{I use completeness not in the mathematical or logical sense, but with the definition that an agent has complete credences if any input into the credence function $Cr:S\rightarrow\mathbb{R}$ yields an output. Equivalently, given any set of known propositions $S$, for any proposition $P\in S$, the agent has $Cr(P)\in\mathbb{R}$.} and consistent\footnote{An agent has a consistent assignment of credences if these assignments abide by the Kolmogorov axioms.} credences. By introducing additional mathematical sophistication, a new model could demand so much calculation from an ordinary agent that it may be impossible to determine if they are truly rational, so this new model will ironically face the same impossible demands on rationality posed by the original problem, just in another form. Hence, introducing mathematical complexity defeats the purpose of the Bayesian model, and those advocating this approach to strengthen the model will also have to account for the limited cognitive capacity of an ordinary agent. Extreme reformulations are therefore an unacceptable solution to the problem, and the only solution is to adopt a moderate approach espoused by the WN formulation.
\section{Conclusion}
This paper has demonstrated that the only solution to the problem of logical omniscience is to adopt the WN formulation of normalisation, which limits the assignment of maximum credence only to logical truths that ordinary agents can know. Adopting weaker formulations like WVN destabilises our conventional system of probability, and also causes new impossible demands on rationality to reappear as a model that is too convoluted to be useful for an ordinary agent.
\pagebreak
\printbibliography
\end{document}